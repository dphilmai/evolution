\documentclass[12pt]{article}
\usepackage{todonotes}
\usepackage[style=authoryear,url=false]{biblatex}
\bibliography{bibevol2.bib}
\bibliography{bibevol.bib}
\usepackage{sgame}
\input{mathinput.tex}
\pgfplotsset{compat=1.11}
\linespread{1.5}
\newcommand{\natnumb}{\mathbb{N}}
\newcommand{\realnumb}{\mathbb{R}}
\presetkeys{todonotes}{color=yellow,size=\scriptsize}{}
\begin{document}
\section{Introduction}
\begin{figure}[h]
\begin{center}
\begin{game}{2}{2} & $S$ & $H$
\\ $S$ &$a,a$ &$b,c$
\\ $H$ &$c,b$ &$d,d$ \end{game}
\label{sh}
\end{center}
\caption{A parametrized form of the Stag hunt game}
\end{figure}
\section{The Stag hunt game}
\todo{Motivation of traditional game theory}
As introduced, the stag hunt game is played by two players who choose their
strategies simultaneoulsy. Both have information about the strategies of the
other player and the payoffs they and their opponents receive. In game theory
such a game is called a \textit{normalform} game. Typically, such a game is
formalized as a Triplet $\Gamma = (I,\mathbb{S},F)$, with the set of players 
$I=\{1,2,...,n\}$, where $n$ is the total number of players, 
the pure strategy space of the game $S = \times_i S_i$
with the pure strategy space $S_i$ of an individual player 
$i \in I$ defined as $S_i = \{1,2,...,m_i\}$, where $m_i$ denotes the total
number of strategies available to player $i$, and the payoff function 
$F: S \rightarrow \realnumb^n$.
As this text focusses on the stag hunt game, which is a normalform game,
this definitons reduce to the following.
By setting $n=2$, we define the two hunters playing SH as $I=\{1,2\}$. A 
player $i \in I$  can choose from a set of pure strategies from his 
pure strategy space, $S_i$. Those strategies are called pure, because they 
differ from the later defined mixed strategies. In the SH game, 
each hunter can choose out of two pure strategies, namely, huntig the stag 
(strategy 1) or hunting hare (strategy 2).
Therefore, the pure strategy space is defined by $S_i = {1,2}$, which results
from definiton ???? by setting $m_i$. In the SH game both players choose from
the same set of stragies such that $S_1 =S_1=S$. Combining the strategy spaces
of the two players with the cartesian product
yields the definiton of the pure strategy space of the game
$\mathbb{S}= S \times S = S^2$.

In SH the hunters do not just hunt for their pleasure \todo{Hunter bewerten 
Ausgang des Spiels}\footnote{Even though the
situation would not differ, aslong as we assume the pleasure hunting a stag
to be higher than hunting hare, since economically payoffs are interpreted as
utility in the theory of households.}, but to seek 
a reward for their loot. This is captured by the definition of an individual
payoff function $F_i:S \rightarrow \realnumb$, which maps for a player $i \in
I$ to every state of the game $s=(s_1,s_2) \in S^2$, where $s_i$ is a 
pure strategy of player $i$, a payoff $F_i(s) \in \realnumb$. So
every outcome of the game, every combination of strategies the players could
individually choose, is defined. In the special case of two player, one 
can define a payoff matrix for player 1 $A \in \realnumb^{2 \times2}$ and for 
player 2  $B \in \realnumb^{2 \times2}$, where $A_{kl} = F_1(k,l)$ and $
B_{kl} = F_2(k,l)$ with the pure strategies $k,l \in S$. Using the 
representation above, one can indentify the payoff matrices for the player as
\begin{align}
        A = \mqty(a && b \\ c && d), \quad B = \mqty(a && c \\ b && d)
        \label{eq:matrix}
\end{align}

In the study of games, one is interested in defining subclasses of games.
For the game here in study the following definition is important: 
\begin{mydef}
        A two player game $\Gamma=(I,S_1 \times S_2, A,B)$ is called symmetric
        if the players of the game have the same strategy space $S_1=S_2=S$ and
        for the payoff matrices the condition $B=A^T$ holds. Therefore, the
        game is well-defined as $\Gamma=(I,S^2,A)$.
        \label{symmetry}
\end{mydef}
Clearly, the SH game is such a game. Both hunters have the same strategies 
available and the payoff matrices defined in \todo{Richtiges Nummerieren der 
Gleichungen} \eqref{eq:matrix} fulfill the required condition in Definition
\ref{symmetry}. Hence, it is irrelevant which player we label 1 or 2.


Usually a game is extended by the possibility of the players to play
\textit{mixed strategies}. Intuitively, in the analogy of SH, one can think of
such strategies as that a hunter decides whether to shoot the stag or the hare
by flipping a coin which shows head or tails, not necessarily with an equal
probabillity.\todo{Interpretation intuitively problematic e.t.c.} Formally, every player of the game assigns a probabillity 
distribution over his pure strategies space.  
A strategy $x_i \in \Delta_i$ of player $i \in I$ 
is called a \textit{mixed strategy}, where $\Delta_i$ is the mixed strategy 
space 
\begin{align*}
        $\Delta_i = \{ x_i \in \realnumb^2 : \sum_{k \in S} x_{ik} = 1, x_{ik} \geq 0 \quad
\forall k \in S\}.
\end{align*}
\todo{Geometric Interpretation as a Simplex}
Since there is no restriction for any players choice of the probabillity 
distribution, in a symmetric game the mixed strategy spaces of the players
also equal. For the SH game this means $\Delta_1 = \Delta_2 = \Delta$.
With this notation a pure strategy can be interpreted as a mixed strategy
which assigns probabillity one to the pure strategy chosen and zero to all
other strategies. This is represented by the unit vectors of the simplex 
$\hat{e}_k \in \Delta$, $\hat{e}_1 = (1,0)^T$ is the pure strategy hunting stag 
and $\hat{e}_2 =(0,1)^T$ denots the pure strategy hunting hare.
Similiar to the pure strategy case, the mixed strategy spaces of the players 
$\Delta$ is combined to the mixed strategy space of the game $\Delta^2 =
\Delta \times \Delta$. From now on $x \in \Delta$ denotes a (mixed) strategy
chosen by player 1 and $y \in \Delta$ a (mixed) strategy of player 2.
Again similiar to the pure strategy case, the mixed strategy payoff 
$\hat{F}_i:\Delta^2 \rightarrow \realnumb$ maps to any state in the mixed strategy
space  $(x,y) \in \Delta^2$ a payoff $\hat{F}_i(x,y) \in \realnumb$.
With the matrix notation this is defined as: 
\begin{alignat*}{2}
        \hat{F}_1(x,y) &= x^T A y \\
        \hat{F}_2(x,y) &= x^T A^T y 
\end{alignat*}

Finally, the SH game is formally defined as a symmetric two-player normal form
game $\Gamma = (I=\{1,2\}, \Delta^2, \hat{F})$.

\subsection{Traditional concepts}
In describing the behavior of agents game theory developed a wide range of 
tools to solve this strategic interactions. \todo{Rationality assumption and
Equilibrium knowledge required for Nash equilibrium}

The \textit{best-reply} for player $i \in I$ to a strategy $y \in \Delta$ 
played by $j \neq i$ is defined as:
\begin{align}
        \beta(y) = \{x \in \Delta: \hat{F}(x,y) \geq \hat{F}(x',y), 
        \quad \forall x' \in \Delta\}
\end{align}
This formally assigns to each strategy of the other player the strategies
of player $i$ resulting in the highest payoff for player $i$. However, a player
must be capable of computing his best-reply to a given strategy.

The most famous and used traditional solution concept, the \textit{Nash 
equilibrium} assumes this kind of capability of each individual players. 
The Nash equilibrium was named by its proposer J. Nash in 1950. \todo{Citation
of Nash and history}

A Nash equilibrium for the SH game is defined as
\begin{mydef}
        A state of $(x^*,y^*) \in \Delta^2$ is called a Nash equilibrium if 
        it holds that\todo{Nicer form}
\begin{itemize}
        \item   $(x^*)^T A y^* \geq x^T A y^*, \quad \forall x \in \Delta$
        \item   $(x^*)^T A y^* \geq (x^*)^T A y, \quad \forall y \in \Delta$.
\end{itemize}
It is called symmetric if $x^* = y^*$. It is a strict Nash equilibrium if 
the inequality is strict.
\end{mydef}
It is equivalent to say that both players play best-replies in a Nash 
equilibrium. One can proof that in every normal form games with mixed 
strategies a Nash equilibrium exists. This existence proof is due to Nash\todo{Citation}.

So, the SH game admits three symmetric Nash equilibria. The first one consists
of both players choosing strategy 1, hunting stag, $(\hat{e}_1,\hat{e}_1) \in
\Delta^2$. The second one is the state of 
both players choosing strategy 2, hunting hare, $(\hat{e}_2,\hat{e}_2)
\in \Delta^2$. Additionally to this Nash equilibria in pure strategies, there 
is a mixed strategy equilibrium. 
\todo{Local shifts, associated payoff matrices}
\subsection{Evolutionary concepts}

\subsection{Equilibrium Selection}
Clearly, game theory has the aspiration to provide advise for agents in 
situations such as that defined above.
As seen in the stag hunt game, the mostly used solution concept, 
the Nash equilibrium, and refinements such as the ESS are
not sufficient to select a unique equilibrium, even in the case of a simple
2x2 game. This does not satisfy game theorists since it is not clear which
equilibrium is finally played by the agents. Or how \cite{weibull} puts it,
this kind of coordination games "caused\footnote{And still causes} game theorists and users of 
noncooperative game theory a fair amount of frustration". 

A closer look at the two pure Nash equilibria in the Stag hunt game 
shows their differences. Considering the normal form representation in figure 
\ref{sh}, the Nash equilibrium where both players choose strategy one has the highest payoff
for both players. It is then said that $(\hat{e}_1,\hat{e}_2)$ 
\textit{Pareto-dominates} $(\hat{e}_2,\hat{e}_2)$. In the other standard 
normal form game describing a social-dilemmata, the Prisoners Dilemma, as 
shown in figure \ref{pd} the Pareto-dominant outcome is not a 
Nash equilibrium\footnote{This is true for one-shot games. 
In repeated games there is the possibility in ending at the Pareto efficient 
outcome.}, because every agent has the incentive to deviate. In the stag hunt
game this is not the case. Every agent plays a best-reply in the Pareto efficient
outcome, but since there is another Nash equilibrium it is not clear 
which one is played based on this criterion alone. Based on \cite{schelling}
one may argue that Pareto-dominance characterizes $(\hat{e}_1,\hat{e}_1)$ 
as a focal point of the game, a solution of the game which makes the most sense
to play but the reasons for this are abstracted away from the strategic form and are consens among
the players. \todo{Focal points definition} However, the equilibrium point
$(\hat{e}_2,\hat{e}_2)$ exhibits the feature of \textit{Risk-Dominance}. 
\cite{seltenharsanyigeneral} defined this selection criterion based on the
risk for the players associated with an equilibrium point. Formally, in the
SH game, 
\begin{mydef}
The Nash equilibrium $x=(\hat{e}_2,\hat{e}_2)$ \textit{risk-dominates} 
         the Nash equilibrium $y=(\hat{e}_1,\hat{e}_1)$, if $d-b > a-c$.
         \label{riskdom}
 \end{mydef}\todo{Maybe change this definition}
 As mentioned in \cite{weibull} a NE risk-dominates, if it is Pareto-efficient
 in the reduced payoff version of the game, associated with the diagonal
 payoff matrix in \eqref{diagmatrix}, as the definition \ref{riskdom} 
 corresponds to $\alpha_1 < \alpha_2$.
So both pure Nash equilibria have a certain appeal for game theorists to be
favored. Indeed, there is no clear consent which equilibrium will be played. 
\cite{aumann} argues that communication before the game would lead to 
an agreement on the Pareto-dominant equilibrium.\todo{Cheap talk: Find the paper
Aumann 1990}
Although Harsanyi and Selten introduced the Risk-dominance criterion in 
\cite{seltenharsanyigeneral}, their general theory favors Payoff-dominance in the
Stag hunt case. On the hand, Harsanyi, as Aumann, later advocates in \cite{harsanyinew} 
for the Risk-dominant criterion. 

The evolutionary approach outlined in the next section may contributes to 
the equilibrium selection problem. 

\section{The Evolutionary Game}
\label{evolutionarysection}
Evolutionary game theory, started by Maynard Smith, has find a lot of 
recognition in the game theory literature. \todo{Applications and stuff}

In an evolutionary game, a population of agents interacts in a 
strategic environment. It is usually assumed that the number of agents is 
large, such that an individual agents decision has a small effect on the
state of the population. This is called a \textit{population game} \cite{sandholm_population_2010}.

An evolutionary stag hunt game then consists of agents in a population who
are randomly matched against each other playing the normal form game 
$\Gamma = (I,\Delta^2,\hat{F})$ discussed before. In a \textit{monomorphic}\footnote{In 
contrast to \textit{polymorhpic} populations where each agent can also 
play mixed strategies} setting, the agents are only able to play pure 
strategies. As both settings do not differ in their main implications, 
monomorphic populations admit a handy interpretation for mixed-strategies\todo{Citation, What is Game Theory Trying to accomplish}.
Let $p_j(t) \geq 0$ be the number of individuals playing strategy $j \in \{1,2\}$
at time $t \in \realnumb$ and let $p(t) = p_1(t) + p_2(t)$ describe all individuals 
in the population\footnote{In the model considered here the population will 
not be allowed to grow and hence $p(t) = P \forall t \in \realnumb$}
then $x(t) = \left(x_1(t),x_2(t)\right)=\left(\frac{p_1(t)}{p(t)},\frac{p_2(t)}{p(t)}\right)$ is called the state vector of the population at
time $t \in \realnumb$. The components of the population state vector represent
the share of agents choosing a specific strategy. 
As the individuals can only choose between strategy one and two it holds that 
$x_2(t) = 1-x_1(t)$. Since $x(t) \in \Delta$ \footnote{There is formally no
difference between population state and mixed strategy} a mixed-strategy can 
be interpreted as a population state in a monomorpic population, where the 
population is divided on different strategies. Solving the interpretation
problem for the population setting this interpretation has little relevance
for the standard case. \todo{Cite Interpretation paper}
In the biological literature\cite{maynard} strategies are interpreted as
phenotypes of the population and the payoffs represents the so-called 
\texit{Darwinian fitness} expressing the expected offsprings for the phenotype
in the population. Some animals in the population then die and are replaced
according to the fitness of the phenotype in the current population
state.\todo{Nicer formulation} 
This interpretation is rather unpractical since economic situations rarely
involve the death of their participiants. Therefore, the literature introduces
\texit{Revision Protocols} to model the adjustment of each agents' strategy.
So the agents in this context are not simply replaced by new ones, but rather
change their strategy according to a certain rule. Nevertheless, the
similiarity to biology stays in the manner that the agents are not
assumed to be the \textit{homo rationalis} game theory usually deals with, but
are "programmed" or "wired" \cite{gintis} to their strategy to the revision rule.
\subsection{Revision Protocols}


\subsection{Replicator Dynamics}
The Replicator Dynamic, which is called the "best know dynamic in
evolutionary game theory" by \cite{sandholm_populaton_2010}, has a wide range of applications
and attractive properties. As discussed above, it can be motivated by 
different revision protocols. Formulated for the stag hunt game, one finds for
the strategies $j \in \{1,2\}$
\begin{alignat}{2}
        \dot{x}_j &= x_j\left(\left(x^T A\right)_j - \left(x^T A x\right)\right) 
        \label{replicator}.
\end{alignat}
$x(t)=(x_1(t), x_2(t)) \in \realnumb^2$ is then called a \textit{dynamical
system} defined by the two differential equations in \eqref{replicator}. For
later definitions I will denote the equations as $\dot{x}(t) = f(x(t))$, where
$f(x(t))$ describes the vector field corresponding to the right side of
\eqref{replicator}. \todo{Introduce the notation and terms of a dynamical system}
The growth of the share $\dot{x}_1(t)$ at time $t \in \realnumb$ depends on the current share in the
population of that strategy and the excess payoff $\bar{F}(x(t)) 
= \hat{F}(x,x) = x^T A x$ that strategy earns 
compared to the average payoff of the entire population. Intuitively, the
share of a strategy in the population increases (decreases) if the strategy
yields a higher (lower) than average payoff. This property of a (deterministic)
evolutionary dynamic is called \textit{Positive correlation} \cite{sandholm}.
Another property of the Replicators Dynamics is the low data requirement. As 
seen in the previous section, it is not necessary to assume that an agent
is able to calculate or gets to know the payoffs of all agent in the population.
It was sufficient to assume that he gets to know the payoff of one player. 
As the agents with such revision protocols are often described as boundedly
rational, \cite{gintis} argues that "this is very misleading, because the real 
issue is the distribution of the information". The argument goes, that even
if they were not "bounded" they could not optimize since they do not have
not more information. 
One drawback of the Replicator Dynamic for certain applications, especially
for biologists, is the lack of mutation, a random error in the replication 
process.
\eqref{replicator} shows that the growth of a strategy not present in a 
population state $x(t)$ is zero. So, a population only consisting of agents
playing strategy one stays in that state forever. 

This leads us to the concept of a fixed point. A point $x^* \in \realnumb^2$ of a dynamic system,
such as the replicator dynamic in \eqref{replicator}, is called a fixed point
if $\dot{x}(t) = f(x^*) = 0$ i.e. it stays in the current state for all $t \in 
\realnumb$. 
It is practical for further use to express the replicator dynamics for the
parametrized SH game as 
\begin{align}
        \dot{x} = f(x) = x^2(a-b+2d-2c) - x^3(a-b+d-c) -x(d-c) 
\end{align}
where f(x) is a polynominal of degree $3$. The (real) roots in the 
of this polynominal correspond to the fixed points. 
For the parameter set $a=5, b=3, c=0, d=2$  the graph is plotted in \ref{polynominal}.
Analyzing \eqref{replicator}, this happens if either a strategy
is not present in the population or the excess payoff $\bar{F}(x)$
of a strategy is zero. In the stag hunt game, the fixed points of the
dynamic coincide with the Nash equilibriums of the base game. In general, 
the replicator dynamic lacks the \textit{Nash stationarity} property discussed
in \cite{sandholm}, and hence there are games with fixed points which are not
Nash.\footnote{For a trivial example, consider the prisoners dilemma in 
\ref{prisoners}. A population only consisting of cooperating agents stays in
that state, which is not a Nash equilibrium.} 
Concerning fixed points, a natural question to ask is if they are stable. 
A rather strong requirement is the concept of \textit{asymptotical stability}.
\begin{mydef}
        A state of a differential equation is called asymptotically stable
        if ....
\end{mydef}\todo{Exact definition}
Hence, the system tends to move back to an asymptotical stable fixed point
once disturbed. 
A useful theorem to check for asymptotical stability is that 
of \cite{hartmanngrobman}. \todo{State the theorem of hartmanngrobman}
Applying this theorem to the replicator dynamic of the stag hunt game, we find
\section{Experimental Evidence}
A third approach beside the traditional and the evolutionary treatment of coordination problems is what \cite{camerer} calls 'fundamental empirical'.
Hence, this section surveys the experimental economic literature regarding 
laboratory coordination games to provide evidence how actual people choose
their strategies in such situations and which factors might influence them. 
Precisely, do people, if they are able to coordinate on an equilibrium at all,
play the risk-dominant or the payoff-dominant equilibrium? And how does their
choice change playing the game repeatedly?
Due to the rich experimental literature on human decisions in social dilemmetas, I will concentrate on studies with frameworks similiar to the evolutionary stag hunt trying to compare the evidence with the results of the theoretical approaches outlined in previous sections. 

The papers of \textcite{van_huyck_tacit_1990} and \textcite{cooper_communication_1992} are 
seen as the first experiments investigating coordination games \parencite{devetag_when_2007}.
In \textcite{van_huyck_tacit_1990}, subjects do not play a stag
hunt, but an order-statistics game. These games differ in that subjects choose
from a broader set of strategies and hence there are more than two equilibria
in the game. Also the games are played in a group and the payoff to all 
players depend on the lowest strategy one of them chose, thus the name 
'weak-link' order statistics game. Nevertheless, the structure is similiar to 
the stag hunt in the way that the equilibria are Pareto-ranked, with one 
'safe' strategy \parencite{devetag_when_2007}. So evidence from this games is likely to 
be transferable to the coordination problem in the stag hunt game. In 
\cite{van_huyck_tacit_1990} subjects play the order-statistics game in groups of 
14-16 for 10 periods. They only received information about their payoff, but 
so they could find out what the lowest choice of one subject in their group was.
Astonishingly, in every experiment the groups converged
to the equilibrium with the lowest payoff. This result were preserved in another 
five periods of this game although the subjects played an alternative payoff
matrix embracing the payoff-efficient equilibrium across all groups. 
Replications of this results have been performed numerously, with varying group
sizes and slightly changed payoff matrices. One may consult \textcite{devetag_when_2007}
for a summary. 
\textcite{cooper_communication_1992} perform a experiment with a standard two-player stag
hunt game\footnote{They call it simple coordination game(SCG)} 
and one augmented by a dominated strategy. They find that 
subjects in randomly matched one-shot games, knowing only about their own
received payoffs, coordination also fails without any preplay communication. 
However, they find that two-way communication, in which both players were
able to send a message to their matched partner containing which strategy
they intend to choose, solves the coordination problem in the SCG. 
This early approaces suggest that coordination failure\footnote{Here in the 
meaning of not being able to coordinate on the payoff-dominant equilibrium} is
common in laboratory \parencite{devetag_when_2007}. It is noteworthy that the studies 
discussed used different matching protocols, fixed matching in groups and 
random matching against one other player, and yet found the same results. 
A 'very likely consequence of this' is the focus on characteristics of the
payoff matrix in stag hunt games' rather than the implementational details' 
such as the matching protocol \parencite{devetag_when_2007}.

The implementation of random matching in a group is the closest to the 
evolutionary setting discussed in section \ref{evolutionarysection}. This 
procedure was conducted by \textcite{battalio_optimization_2001}. Subjects 
played three stag hunt games with the typical symmetric pure strategy
equilbrium and the symmetric mixed strategy equilibrium at $(0.8,0.2) \in
\Delta$. Figure \ref{fig:payoffbattalio} show the payoff matrices of the
games $2R, R$ and $0.6R$ used. The names become clear when observing the 
\textit{optimization premium} of the games. While controlling for the basin of 
attraction of the equilibria, i.e. the mixed equilibrium is equal across all games,
they want to explore the effect of an increase in the "premium for playing
a best-response" \parencite[751]{battalio_optimization_2001}. 
\begin{figure}[h]
        \label{fig:payoffbattalio}
\caption{Payoff matrices of games in \textcite{battalio_optimization_2001}}
\begin{alignat*}{3}
        A_{2R} &= \mqty(45 & 0 \\ 35 & 40) \quad & &A_{R} = \mqty(45 & 0 \\ 
        40 & 20) \quad & &A_{0.6R} = \mqty( 45 & 0 \\ 42 & 12) \\
        \delta_{2R} &= 50  \quad & &\ \delta_{R} = 25 \quad & &\ \delta_{0.6R} = 15
\end{alignat*}
\end{figure}
Therefore they define the optimization premium $r(y)$ of a game $j$ as the 
differece in payoffs choosing the pure strategies $e_1$ and $e_2$ 
while expecting the opponent to play a strategy $y=(q,1-q) \in \Delta$:
\begin{align}
        r_j(y)= \hat{F_j}(e_1,y) - \hat{F_j}(e_2,y) = \delta_j(q-q^*),
\end{align}
with the \texit{optimization premium parameter} $\delta_j$. In the notation
parametrized stag hunt game $\delta_j = a - c + d - b$.
The parameters for the payoff matrices are reported in figure \ref{fig:payoffbattalio}. 
First of all, they find support for their hyptohesis that in games with a 
larger optimization premium subjects coordinated less frequently on the 
payoff-dominant equilibrium. 
Contrary to game $0.6R$, where only one cohort coordinated on the risk-
dominant, in $2R$ no cohort and in $R$ only one cohort converged to the 
payoff-dominant equilibrium. The replicator dynamic does not offer a
a description of this behavior, since convergence to a equilibrium only 
depends on its basin of attractions, which do not differ for the three games, 
and the initial conditions. Whereas "all 24 cohorts start in the basin of attraction
of the risk-dominant equilibrium" \parencite{battalio_optimization_2001}, 
all should converge to the risk-dominant equilibrium according to the dynamic.
In the most cases this is true, with only three exception in game $0.6R$ 
and two in $R$.
But, for the sake of completness, a crossing of the  
"best-response separatrix" \parencite{battalio_optimization_2001} i.e. 
mixed-strategy equilibrium, can not happen in the deterministic replicator 
dynamic.
Furthermore, as they conjectured, a larger
optimization premium increases the speed of convergence. Hence, a coordination
on a equilibrium was achieved fastest in the $2R$ game. 
This is consistend with a postulated replicator dynamic for this games. 
Starting from \ref{replicator}, one can derive that the dynamics of this games
are the same up to a change in speed, proportional to the ratio of 
optimization premia of the games. In game $R$ the change in the population share
playing strategy one is, thus, half the speed of $2R$ and four thirds of $0.6R$,
$\dot{x}_{R} = 0.5 \dot{x}_{2R} = \frac{4}{3}\dot{x}_R$.




    













\section{Conclusion}
\printbibliography
\end{document}







