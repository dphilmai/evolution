\documentclass[11pt]{article}
\usepackage{pgf}
\usepackage{todonotes}
\usepackage{float}
\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[hypcap=true]{caption}
\usepackage{subcaption}
\usepackage[backend=biber,style=apa,url=false]{biblatex}
\DeclareLanguageMapping{american}{american-apa}
\bibliography{bibevol4.bib}
\usepackage{sgame}
\input{mathinput.tex}
\pgfplotsset{compat=1.11}
\linespread{1.5}
\newcommand{\natnumb}{\mathbb{N}}
\newcommand{\realnumb}{\mathbb{R}}
\presetkeys{todonotes}{color=yellow,size=\scriptsize}{}
\begin{document}
\input{titlepage.tex}
\pagenumbering{roman}
\tableofcontents
\newpage
\listoffigures
\newpage
\pagenumbering{arabic}
\section{Introduction}
The beginnings of game theory date back to Johann von Neumann in 1928 
who developed a mathematical framework
for modelling situations with strategic interactions of rational agents 
\parencite{v._neumann_zur_1928}.
In a situation of strategic interaction, the outcome for an agent does not
only depend on his choice, but also on the choice of the other agents.
In economics, such situations often present themselves as a social dilemma,
a situation where a group of agents is prevented to achieve the best
outcome for the group, because of strategic considerations of the individual
agent.
Asking a social scientists about \textit{the} example of a social dilemma, 
one usually gets to hear the story about two prisoners, arrested and 
seperately interrogated by an authority. Lacking the amount of evidence to sue 
the criminals for serious crimes, the authority offers them a deal 
simultaneously.
A confession grants a prisoner with amnesty and hence he escapes the punishment
for the serious crimes. However, if both choose to confess, they receive the
full sharpness of the law. Not admitting the crimes would leave the authority
with too little evidence, resulting only in a sentence for minor crimes.
This story, developed by Albert Tucker in 1950 and suitably named Prisoner's
Dilemma (PD) is usually represented in a table, as shown in figure \ref{fig:pd}, with
the options for each prisoners denoted by C, remaining silent, and D, admitting
their crimes. 
The total amount of years for the group is lowest if both do not confess, 
but each individual prisoner has the 
incentive to take the deal of the authority as this leaves him, independently
of what his fellow prisoner does, with a lower time in prison. The dilemma 
clearly consists of the conflict between individual interest and 
the benefit of the group \parencite{skyrms_stag_2004}. However, there is 
another ``story that became a game'' \parencite[1]{skyrms_stag_2004}, 
which describes a different, but as Skyrms argues, 
underrepresented social dilemma. In \textit{A Discourse on Inequality}, 
Rousseau outlines the situation of hunters, heading out to hunt 
stag. However, if a hare runs past a hunter, he might consider to
shoot the hare, banishing all stags in the forest. The total reward for
the group is surely lower, but it is rationale for the single hunter if he
expects the others to act the same, given the opportunity. In contrast
to PD, it is not the hunters individual interest that is competing with the
benefit of the group, but the uncertainty about what the other hunters choose
to do. In fact, none of the hunters would have bad feelings about his choice
not to shoot a hare when returning with a stag, as his personal benefit is 
greater with a stag as bounty. Figure \ref{fig:sh} represents this dilemma 
for two hunters. When both coordinate on stag hunting, denoted by S, they receive 
$a$. If one of them decides to shoot the hare, he gets $c$, whereas
the other hunter is left out in the cold with just $b$. Coordination on hare
hunting yields both with $d$. The parameters are usually assumed to fulfill
$a > c \geq d >b$. In game theory, this type of situation is called
a coordination game. Since in the outcomes the hunters coordinated, either
on hunting stag or hare, they have no incentive to change their action. 
Both outcomes are what game theory calls a Nash-equilibrium, originally
formulated by John Nash in 1950 \parencite{nash_equilibrium_1950}. In a 
Nash equilibrium all players of a game choose a best reply against
the strategies of the other players, so that none of them has an incentive
to deviate unilateral from his decision. Although this is the mainly used
solution concept, it does not give a definite answer in the stag hunt game.
One might argue that both hunters can speak to each other beforehand and
assure each other that hare hunting is more attractive to both them. However,
argues that empirical observation and theory suggest that the problem is 
not solved that easily \parencite[337]{camerer_behavioral_2003}. 
Furthermore, in a wide range of economic applications interaction of agents
happen repeatedly. Analyzing repeated prisoner dilemmas, it turns 
out that the resulting game can actually be interpreted as a stag-hunt game with
two Nash equilibria \parencite{skyrms_stag_2004}.
This connection between this two social dilemmas advocates that
the selection of equilibria should be investigated more deeply. 
An interesting approach to the equilibrium selection problem comes from
biology. Evolutionary game theory was pioneered by Maynard J. Smith and George
R. Pricein their work on the conflict of animals in populations 
\parencite{smith_lhe_1973}. Started as a refinement of the Nash equilibrium,
the concept of an evolutionary stable strategy, various other authors, for 
example \textcite{taylor_evolutionary_1978}, \textcite{hofbauer_note_1979} and
\textcite{zeeman_dynamics_1981}, developed a `time dependent dynamical
extension of game theory'' \parencite[55]{hanauske_evolutionare_2011}.
In contrast to traditional game theory, the evolutionary approach  models
 agents in a large population, interacting repeatedly in a strategic environment. 
Furthermore, the agents are not assumed to be rational, but follow a specific 
rule, updating their strategy. 
As outlined in the rest of this text, the
evolutionary approach offers an possible answer to the coordination
problem in the stag hunt game.

The thesis is organized as follows. Section 2 introduces the 
framework of traditional game theory for the stag hunt game and discusses
the traditional approach to the equilibrium selection problem. Section 3
motivates evolutionary game theory and presents the replicator dynamic. 
Later on, it will be discussed how the evolutionary approach selects
between multiple equilibria. Section 4 demonstrates the effect of a
network externality in the underlying framework. Looking for evidence
how real people play the stag hunt game, section 5 considers experimental
literature on the topic. Section 6 concludes and shows application of
evolutionary game theory and the stag hunt game in economics.

\begin{figure}[h]
\begin{center}
\begin{game}{2}{2} & $S$ & $H$
\\ $S$ &$a,a$ &$b,c$
\\ $H$ &$c,b$ &$d,d$ \end{game}
\end{center}
\caption[Stag hunt game]{A parametrized representation of the Stag hunt game, $a>c\geq d >b$}
\label{fig:sh}
\end{figure}
\begin{figure}[h]
\begin{center}
\begin{game}{2}{2} & $C$ & $D$
\\ $C$ &$-1,-1$ &$-4,0$
\\ $D$ &$0,-4$ &$-3,-3$ \end{game}
\end{center}
\caption[Prisoner's Dilemma]{A parametrized representation of the prisoners dilemma}
\label{fig:pd}
\end{figure}
\section{The Stag hunt game}
\label{sec:traditional}
\todo{Motivation of traditional game theory}
As introduced, the stag hunt game (SH) is played by two players who choose their
strategies without knowing each others choice.
Both have information about the strategies of the
other player and the payoffs they and their opponents receive. In game theory
such a game is called a \textit{normalform} game with complete information. 
Typically, such a game is formalized as a Triplet $\Gamma = (I,\mathbb{S},F)$, 
with the set of players $I=\{1,2,...,n\}$, where $n$ is the 
total number of players. The pure strategy space of the game 
$\mathbb{S} = \times_i S_i$ with the pure strategy space $S_i$ of an 
individual player  $i \in I$ defined as $S_i = \{1,2,...,m_i\}$, where $m_i$ 
denotes the total number of strategies available to player $i$. The payoff 
function of the game is denoted by $F: S \rightarrow \realnumb^n$.
As this text focuses upon the stag hunt game, these definitions reduce to the 
following.
By setting $n=2$, we define the set of two hunters playing SH as $I=\{1,2\}$. A 
player $i \in I$  can choose from a set of pure strategies from his 
pure strategy space, $S_i$. The strategies are called pure to distinguish
them between the later introduced mixed strategies. 
In the SH game, each hunter can choose one out of two pure strategies, namely, 
huntig the stag (strategy 1) or hunting hare (strategy 2).
Therefore, the pure strategy space is defined by $S_i = \{1,2\}$, which results
by setting $m_i=2$. In the SH game both players choose from
the same set of strategies such that $S_1 =S_2=S$. Combining the strategy spaces
of the two players with the cartesian product
yields the definition of the pure strategy space of the game
$\mathbb{S}= S \times S = S^2$. Games played by two players with two strategies
each, are often said to be 2x2 games.    
In the analogy of the story Rousseau told, the hunters do not just hunt
for their pleasure, but to sell their loot for a reward. 
This is captured by the definition of an individual payoff function for player
i, $F_i:S \rightarrow \realnumb$, which maps a payoff $F_i(s) \in \realnumb$ 
to every state of the game $s=(s_1,s_2) \in S^2$, where $s_i$ i s a pure
strategy of player $i$.\todo{this sentence}
The payoffs are usually interpreted as \textit{Von-Neumann} utility when 
played by individuals or households and are in terms of earnings regarding 
firms. 

In the special case of two player, one 
can define a payoff matrix for player 1, $A \in \realnumb^{2 \times2}$ and for 
player 2,  $B \in \realnumb^{2 \times2}$, where $A_{kl} = F_1(k,l)$ and $
B_{kl} = F_2(k,l)$ with the pure strategies $k,l \in S$. Using the 
representation in \ref{fig:sh}, one can identify the payoff matrices for 
the player as
\begin{align}
     A = \mqty(a && b \\ c && d), \quad B = \mqty(a && c \\ b && d)
        \label{eq:matrix}
\end{align}
For the game here in study, the following definition is important: 
\begin{mydef}
        A two player game $\Gamma=(I,S_1 \times S_2, A,B)$ is called symmetric
        if the players of the game have the same strategy space $S_1=S_2=S$ and
        for the payoff matrices the condition $B=A^T$ holds. Therefore, the
        game is well-defined as $\Gamma=(I,S^2,A)$.
        \label{symmetry}
\end{mydef}
Clearly, the SH game satisfies this condition. Both hunters have the same 
strategies available and the payoff matrices defined in 
\eqref{eq:matrix}  are the transpose of the other.
Hence, it is irrelevant which player is labeled as player 1 or 2, because 
they are identical with respect to their strategies and payoffs.
Usually a game is extended by the possibility of the players to play
\textit{mixed strategies}. 
The first intuition of a mixed strategy in the SH game would be hunters
choosing whether to shoot the stag or the hare by using a randomization
tool such as flipping a coin. However, randomization is not really
satisfying in the most applications \parencite{radner_private_1982}. An
alternative interpretation, outlined by Rubinstein, is that mixed 
strategies are actually deterministic, but seem to be random because they 
depend on not modelled private information of the individuals 
\parencite[914]{rubinstein_comments_1991}. In the evolutionary context,
discussed in section \ref{sec:evolutionarystaghunt}, mixed strategies have
a different, unproblematic interpretation. Besides the interpretation problem,
mixed strategies are appealing because they ensure the existence
of a Nash equilibrium, which is defined later in this text.
Formally, every player of the game assigns a probability distribution over
his pure strategies space. A strategy $x_i \in \Delta_i$ of 
player $i \in I$ is called a \textit{mixed strategy}, where $\Delta_i$ is 
the mixed strategy space 
\begin{align*}
        \Delta_i = \{ x_i \in \realnumb^2 : \sum_{k=1}^2 x_{ik} = 1, x_{ik} \geq 0 \quad
\forall k \in S\}.
\end{align*}
In a symmetric game the mixed strategy spaces of the players
equal, as both players are identical in assigning probabilities to their
pure strategies. For the SH game this means $\Delta_1 = \Delta_2 = \Delta$.
With this notation a pure strategy can be interpreted as a mixed strategy
which assigns probability one to the pure strategy chosen and zero to all
other strategies. This is represented by the unit vectors 
$e_k \in \Delta$, $e_1 = (1,0)^T$ is the pure strategy hunting stag 
and $e_2 =(0,1)^T$ denotes the pure strategy hunting hare.
The combined mixed strategy space of the game is $\Delta^2 = \Delta \times
\Delta$.
From now on $x \in \Delta$ denotes a (mixed) strategy
chosen by player 1 and $y \in \Delta$ a (mixed) strategy of player 2.
Again similar to the pure strategy case, the mixed strategy payoff 
$\hat{F}_i:\Delta^2 \rightarrow \realnumb$ maps to any state in the mixed strategy
space  $(x,y) \in \Delta^2$ a payoff $\hat{F}_i(x,y) \in \realnumb$.
With the matrix notation this is defined as: 
\begin{alignat*}{2}
        \hat{F}_1(x,y) &= x^T A y \\
        \hat{F}_2(x,y) &= x^T A^T y 
\end{alignat*}
Summarizing the notation, the SH game can be defined as a symmetric two-player
normal form game $\Gamma = (I=\{1,2\}, \Delta^2, \hat{F})$.

\subsection{Solution concepts}
\label{sec:traditionalconcepts}
In describing the behavior of agents, game theory developed a wide range of 
tools to solve this strategic interactions. First of all, for each individual
a best-reply is defined, simply describing the strategies available to him 
which yield the highest payoff against a given strategy of the other player.
Formally, the \textit{best-reply} for player $i \in I$ to a 
strategy $y \in \Delta$ played by $j \neq i$ is defined as:
\begin{align}
        \beta_i(y) = \{x \in \Delta: \hat{F}(x,y) \geq \hat{F}(x',y), 
        \quad \forall x' \in \Delta\}
\end{align}
Based on agents choosing a best-reply, the mostly used solution concept of
a normalform game is the \textit{Nash equilibrium}.
The Nash equilibrium was named by its proposer J. Nash in 1950 
and due to its prominence it is sometimes simply referred to as equilibrium.
In the following, the definition of a Nash equilibrium in a symmetric two
player game is stated. 
\begin{mydef}
        \label{def:nashequilibrium}
        A state of $(x^*,y^*) \in \Delta^2$ is called a Nash equilibrium if 
        it holds that
        \begin{alignat*}{2}
                (x^*)^T A y^* &\geq x^T A y^*, \quad \forall x \in \Delta \\
        \end{alignat*}
It is called symmetric if $x^* = y^*$. A Nash equilibrium is called a 
strict Nash equilibrium if the inequality is strict.
\end{mydef}
Only one equality is needed in a symmetric game as strategy space and
payoff function of each player are identical.
Equivalently to the definition, one can say that a strategy combination 
is a Nash equilibrium if all players choose a best-reply in that strategy 
combination and so not one player has the incentive to deviate from his 
choice, given the choice of the other players.
In \textcite{nash_equilibrium_1950}, Nash proofed the existence of such 
an equilibrium in any normal form game with mixed strategies. 
The SH game admits three symmetric Nash equilibria. The first one consists
of both players choosing strategy 1, hunting stag, $(e_1,e_1) \in
\Delta^2$, where both players receive the payoff $a$. 
None of both has an incentive to change his decision. For both
playing the pure strategy 1 is a best-reply because a unilateral deviation 
leads to the payoff $c \leq a$.
The second equilibrium is the state of 
both players choosing strategy 2, hunting hare, $(e_2,e_2)
\in \Delta^2$. Again, by definition, both play a best-reply and a unilateral 
deviation of a player results in a lower payoff $b<d$.
Additionally to this Nash equilibria in pure strategies, there is a symmetric
mixed-strategy equilbrium where both players assign a probability 
$q^*=(\alpha_2,\alpha_1 + \alpha_2)$ to strategy 1. 
To see this, a player is indifferent between choosing one of his pure
strategies against $y$, $\hat{F}(e_1,y) = \hat{F}(e_2,y)$, exactly
when $y=(q^*,1-q^*)^T$.

For further analysis it is convenient to use the fact that Nash equilibria 
are only defined by a difference inequality. 
Hence, any affine transformation of the payoff matrix does not change the 
best-replies of the players and so does not effect the set Nash equilibria 
of the game \parencite[17-19]{weibull_evolutionary_1997}. 
Furthermore, adding a constant to every entry in a column of the payoff matrix, 
called a \textit{local shift}, does also not effect the payoff differences
a player considers when valuating which strategy is better against a given
strategy of the other player.
Formally, let $A_{ij}$ denote the elements of the player's payoff matrix. 
A local shift to column $j^*$ transform this payoff matrix into the payoff 
matrix $A^*$:
\begin{align*}
        A^*_{ij} =
        \begin{cases}
                A_{ij} + v & \text{for}\ j=j^*, v \in \realnumb \\
                A_{ij}
        \end{cases}
\end{align*}
Applying this to the stag hunt game one can use two local shifts to turn 
the payoff matrix into a diagonal matrix with the elements $A_{11}=\alpha_1$ 
and $A_{22}=\alpha_2$, where $\alpha_1=a-c$ and $\alpha_2=d-b$. 
\textcite[28]{weibull_evolutionary_1997} groups all symmetric 2x2 games into 
four categories with different equilibrium properties. 
The stag hunt game is considered as a coordination 
game, with $\alpha_1, \alpha_2 > 0$ and in contrast to the class of games
with dominated strategies, for example the PD, the solution of the game
is not so obvious as there are multiple Nash equilibria.
That the Nash equilibrium does not point to a unique solution for the stag
hunt game will be discussed in section \ref{sec:equilibriumselection}.

\subsection{Evolutionary concepts}
The stag hunt game has multiple Nash equilibria. Asking which equilibrium
will be played, seems to be reasonable. Answering this question, game 
theorists tried to construct refinements of the Nash equilibrium in cases where 
some equilibria seemed to be unconvincing. Motivated by the context of 
evolution in animal populations \textcite{smith_lhe_1973} constructed the 
refinement of a \textit{Evolutionary stable strategy} (ESS).  
In the biological context, a game is played 
by a large population randomly matched against each other.
An ESS is a strategy that, once adopted by the whole population, cannot
be invaded by any other strategy. 
This definition of an ESS makes clear that it is actually a refinement of the 
Nash equilibrium.
\begin{mydef}
        A strategy $x \in \Delta$ is called a evolutionary stable strategy 
        (ESS) if it holds that
        \begin{alignat}{2}
                \label{eq:essstrict}
                \hat{F}(x,x) &\geq \hat{F}(y,x) \forall y \\ 
                \hat{F}(y,x) &= \hat{F}(x,x) \Rightarrow  
                \hat{F}(y,y) < \hat{F}(x,y) \forall y \neq x \label{eq:essstrict2}
        \end{alignat}
\end{mydef}
Equation \eqref{eq:essstrict}
requires an ESS $x$ to be a better reply to itself than any other strategy $y$.
If there exists another strategy $y$ with the same payoff, equation 
\eqref{eq:essstrict2} demands that it is better to choose $x$ against $y$ than
choosing $y$ against itself.
Indeed, seen in \eqref{eq:essstrict}, an ESS is used in a Nash equilibrium as 
it needs to be a best-reply to satisfy the ESS condition. In addition, 
a strategy used in a strict Nash equilibrium is always an ESS, 
since it directly satisfies 
\eqref{eq:essstrict}. 
In the stag hunt game, not all strategies are evolutionary stable.
The pure strategies $e_1$ and $e_2$ are evolutionary stable,
since both are strict symmetric Nash equilibria and so 
satisfy condition \eqref{eq:essstrict}. 
The mixed strategy Nash equilibrium is not an ESS.  
Let again $y=(q^*,1-q^*)^T$, with $q=\frac{\alpha_2}{\alpha_1+\alpha_2}$.
Considering for example the pure strategy $e_1$, 
equation \ref{eq:essstrict} is $\hat{F}(y,e_1)= \hat{F}(y,y)$.  
But as equation \eqref{eq:essstrict2} is violated,
$\hat{F}(y,e_1) = \frac{\alpha_1 \alpha_2}{\alpha_1+\alpha_2}
< \alpha_1 = \hat{F}(e_1,e_1)$, the Nash equilibrium with mixed strategies 
is not an ESS.

In section \ref{sec:replicatordynamic}, the usefulness of the ESS as solution
concept for the dynamic framework of evolutionary game theory will come 
into sharper relief.

\subsection{Equilibrium Selection}
\label{sec:equilibriumselection}
Clearly, game theory has the aspiration to provide a unique solution in every
strategic interaction.
As seen in the stag hunt game, the mostly used solution concept, 
the Nash equilibrium, and refinements such as the ESS are
not sufficient to select a unique equilibrium, even in the case of a simple
2x2 SH game. This does not satisfy game theorists, since it is not clear which
equilibrium is finally played by the agents. Or how 
Weibull puts it, this kind of coordination games 
``caused\footnote{And still causes.} game theorists and users of 
noncooperative game theory a fair amount of frustration'' 
\parencite[30]{weibull_evolutionary_1997}. 

A closer look at the two pure Nash equilibria in the Stag hunt game 
shows their difference. Considering the normal form representation in figure 
\ref{fig:sh}, the Nash equilibrium, in which both players choose strategy one 
has the highest payoff
for both players. $(e_1,e_2)$ is then said to 
\textit{payoff-dominate} or to \textit{Pareto-dominate} 
$(e_2,e_2)$. Equivalently, the equilibria are said to be
\textit{Pareto-ranked}.
In the Prisoners Dilemma, briefly described in the introduction, 
the payoff-dominant outcome is not a 
Nash equilibrium, due to the fact that every agent has the incentive to 
deviate and accept the deal of the authority.
Contrary to that, in SH, every agent plays a best-reply 
in the Pareto efficient outcome, but since there is another Nash equilibrium 
it is not clear which one is played based on equilibrium play alone. 
Following \textcite[57]{schelling_strategy_1960}, one may argue that 
Pareto-dominance characterizes $(e_1,e_1)$ as a focal point, 
an outcome of a game that is psychologically prominent, of 
the game, helping the players to coordinate on this outcome.
However, the equilibrium point $(e_2,e_2)$ exhibits the feature 
of \textit{Risk-Dominance}. 
\textcite{harsanyi_general_1988} defined this selection criterion 
based on the
risk for the players associated with an equilibrium point. Formally, in the
SH game, 
\begin{mydef}
The Nash equilibrium $x=(e_2,e_2)$ \textit{risk-dominates} 
the Nash equilibrium $y=(e_1,e_1)$, if $(d-b)^2 > (a-c)^2$.
         \label{eq:riskdom}
 \end{mydef}
The squares on the left and the right side of the inequality are named
\textit{Nash products}.
As mentioned in \textcite{weibull_evolutionary_1997} a NE risk-dominates, 
if it is Pareto-efficient  in the reduced payoff version of the game, 
associated with the diagonal
payoff matrix, as the definition \eqref{eq:riskdom} 
corresponds to $\alpha_1 < \alpha_2$.
So both pure Nash equilibria have a certain appeal for game theorists to be
favored as an outcome of the game. The question is, do people rather play
safe and coordinate on the hare hunting equilibrium, terminating in a social
dilemma as there exists an outcome in which everyone of them is better off.
Or are they able to recognize the Payoff-dominance of stag hunting, 
trusting each other not to react to a hare passing by.
Indeed, there is no consensus which equilibrium will be played. 
Although Harsanyi and Selten introduced the Risk-dominance criterion in 
\textcite{harsanyi_general_1988}, their general theory of equilibrium 
selection favors Payoff-dominance in the Stag hunt case. They argue that
``if each player knows the other to be fully rational [...] they should trust
each other to be fully rational'' \parencite[89]{harsanyi_general_1988}.
Further on, Harsanyi and Selten expect players to coordinate on the
Pareto-dominant equilibrium if they are allowed to communicate beforehand,
sometimes referred to as \textit{cheap talk} when it does not directly affect
the payoffs \parencite[104]{farrell_cheap_1996}, 
because ``an agreement to do so is self-stabilizing''. 
In contrast to that,
\textcite{aumann_nash_1990} points out, that a message from player 1 to 
player 2, saying that he intends to coordinate on the Payoff-dominant 
equilibrium, does not in general contain useful information for player 2. 
Indeed, when $c$ is strictly greater than $d$, a player wants the opposite 
player to choose strategy 1, independently of his choice.
Despite this argument, 
\textcite[114]{farrell_cheap_1996} expect that 
``cheap talk will do a good deal to
bring Artemis and Calliope\footnote{The names
\textcite{farrell_cheap_1996} used for player 1 and player 2.} 
to the stag hunt''. The experimental evidence will give a hint how cheap
talk affects real players. 
``Convinced by Aumann's argument'', Harsanyi formulated 1995 a new theory
of equilibrium selection, solely focussing on risk-dominance as selection
criterion, hence favorising the hare equilibrium in SH game 
\parencite[92,94,96]{harsanyi_new_1995}. 

Even though the evolutionary refinement ESS, simply coinciding with
the pure equilibria, does not help as a selection
criterion in this case, the dynamic evolutionary framework, specified in the
next section, gives an answer to the question which equilibrium is chosen. 

\section{The Evolutionary Game}
\label{sec:evolutionarystaghunt}
Evolutionary game theory, started by Maynard Smith, has find a lot of 
recognition in the game theory literature. 
In an evolutionary game, a large population of agents interacts in a 
strategic environment. It is usually assumed that the number of agents is 
large, such that an individual agents decision has a small effect on the
state of the population. 
An evolutionary stag hunt game consists of agents in a population who
are randomly matched against each other playing the normal form game 
$\Gamma = (I,\Delta^2,\hat{F})$ discussed before. In a \textit{monomorphic}\footnote{
Opposed to \textit{polymorhpic} populations where each agent can also 
play mixed strategies.} setting, the agents are only able to play pure 
strategies. Let $p_j(t) \geq 0$ be the number of individuals in the population
playing strategy $j \in \{1,2\}$ at time $t \in \realnumb$ and 
let $P = p(t) = p_1(t) + p_2(t)$ describe all individuals 
in the population\footnote{In the model considered here the population will 
not be allowed to grow and hence $p(t) = P\  \forall t \in \realnumb$.}
then $\vec{x}(t) = \left(x_1(t),x_2(t)\right)
=\left(\frac{p_1(t)}{p(t)},\frac{p_2(t)}{p(t)}\right)$ 
is called the state vector of the population at
time $t \in \realnumb$. The components of the population state vector represent
the share of agents choosing a specific strategy. 
As the individuals can only choose between strategy one and two in our example,
it holds that  $x_2(t) = 1-x_1(t)$. Since $x(t) \in \Delta$ a 
mixed-strategy\footnote{There is formally no difference between population 
state and mixed strategy.} can 
be interpreted as a population state in a monomorphic population, where the 
population is divided on playing different pure strategies. 
Solving the interpretation problem for the population setting this 
interpretation has little relevance for the traditional case 
\parencite[914-915]{rubinstein_comments_1991}. 
Out of convenience, let the expected payoff 
to a strategy $i \in \Delta$ of an individual when randomly matched
against another individual in the current population state $\vec{x}^t$ 
be denoted by $\hat{F}_i(\vec{x}) = \hat{F}(e_i,\vec{x})$ in the following.
The next section will motivate \textit{revision protocols}, a rule to
which agents are ``programmed'' or ``wired'' \parencite{gintis_game_2000}, 
opposed to any rationality or ability assumption. 
\subsection{Revision Protocols}
\label{sec:revisionprotocols}
With that in mind, this section motivates the use of revision protocols to
model the behavior of agents.
As hinted, in evolutionary game theory, agents are usually not assumed to be 
able to calculate a best-reply, nor are they observing the information relevant
to perform that calculations. This is formalized as a behavioral rule called
the revision protocol. The following derivation of the mean dynamic is due to 
\textcite{sandholm_population_2010}. Typically it is 
assumed that agents have an inner alarm clock which rings at a rate R 
following an exponential distribution. Of course, this does not translate 
to economic applications in individuals having literal clocks around their
wrist, but illustrates the idea of a randomly occuring chance to reconsider
their strategy choice.
The agents' clocks are assumed to be independent of each other such that
individuals chance to revise does not dependent on others.
Whenever the clock of an agent rings, he
receives a revision opportunity, which means that he changes his strategy
with the probability $\frac{\rho_{ij}}{R}$, where
$\rho_{ij}(F_i(\vec{x}),\vec{x})$ is called the conditional switch rate,
representing the rule by which agents change from strategy 
$i$ to strategy $j$. 
In a two-strategy game there are two conditional switch rates, $\rho_{12}$ 
describing rate of switching from strategy 1 to 2 and $\rho_{21}$, 
rate for switching from strategy 2 to 1. For $\rho_{ii}$ no actual switch 
happens. This switch rates can have various forms and therefore on the one 
side describe different behavior of agents and on the other side result in 
different dynamics for the population game.
The dynamics are derived as follows:
Every agent receives $R dt$ revision opportunities, i.e. the clock rings, 
in an time interval $[0,dt]$ as it follows a Poisson distribution with
mean $Rdt$ \parencite[123]{sandholm_population_2010}. 
If the population in the stag hunt game is at state $\vec{x}$, the number 
of agents currently playing strategy $i$ receiving a revision opportunity 
during the time interval of length $dt$ is $Px_i R dt$. As 
\textcite{sandholm_population_2010} argues, this is an approximation because
the state of the population $\vec{x}$ may change during the time interval $dt$.
With the probability for an agent playing strategy $i$ switching to strategy
$j$, one gets for the expected number of switches for strategy $i$ during 
$[0,dt]$ across the population,  $P x_i \rho_{ij} dt$. 
In a two strategy game, the change in the number of agents in the population 
playing strategy 1 is determined by agents switching to strategy 2 
and agents with strategy 2 switching to 1. Hence, one gets:
\begin{align} 
        Pdx_1 =  \underbrace{-Px_1 \rho_{12}dt}_{\text{Switches from 1 to 2}} 
        + \underbrace{Px_2 \rho_{21}dt.}_{\text{Switches from 2 to 1}}
\end{align}
Dividing by $P$ and $dt$ leaves us with the differential equation for
the change in the share of agents playing strategy 1, 
$\dot{x_1} =\frac{dx_1}{dt}$. 
The sum of the time derivatives of population shares must equal zero and so
$\dot{x}_2(t) =- \dot{x}_1(t)$.
There are different plausible revision protocols studied in the literature. 
\textcite[128,129,178]{sandholm_population_2010} discusses various forms of 
this revision protocols, such as logit choice, comparison to average payoff 
and best response 
protocol, all with different properties concerning informational 
burden and the
properties the dynamics have. With an eye to the implied dynamic, the 
\textit{pairwise proportional imitation} protocol will be outlined . 
It assumes, that whenever an
agent receives a revision opportunity he randomly gets to know 
another agent's strategy and its payoff received in the current 
state of the population. 
The switching probability of an agent is proportional to the 
excess payoff the other agent had over his strategy. This is formalized as 
\begin{align}
        \label{eq:pairwiseproportionalimitation}
        p_{ij}(F_i(\vec{x}),\vec{x}) =
                \begin{cases}
                        x_j(F_j(x) -F_i(x)) &\ , \text{for } F_j(x) - F_i(x) > 0 \\
                        0 &\ , \text{else}
                \end{cases}
\end{align}
As a metaphor illustrating this revision protocol, I want to tell the 
story of traders on a marketplace.
Suppose the traders interact on a market selling some 
good, implicitly playing a game with each other. The strategies may be the
way they organize their market stall or what position on the market they 
choose. Each trader usually does not
observe the strategies other traders use to sell things, because the market
place is large and the bustle going on complicated. 
So a trader is usually committed to a strategy he got to know a while ago.
However, occasionally as he returns to his tavern, he sometimes gets to meet 
another trader. Being proud of their earnings, they boast about their 
individual strategy. In this way, they get to know
other traders strategies and as they all dream of a live in pageantry,
they might imitate the other traders. 
However, they are more likely to imitate traders with a higher payoff 
compared to theirs, as such traders feel superior and thus, are the 
loudest in the tavern. 

Plugging the conditional switch rate of the pairwise imitation protocol 
\eqref{eq:pairwiseproportionalimitation} into the equation for the change 
of the population share playing strategy 1, $\dot{x}_1(t)$, leads to the 
\textit{replicator dynamics}:
\begin{alignat}{2}
        \label{eq:replicatorrev} 
        \dot{x}_1 &= 2 x_1 x_2 (F_1(x) - F_2(x)) \\
                  &= 2 x_1 ((1-x_1) F_1(x) - x_2 F_2(x)) \\
                  &= 2 x_1 (F_1(x) - \bar{F}(x)) 
\end{alignat}
This equation fully describes the evolution of the population in time.
Properties and the connection to the stage game are discussed in the next
section.

\subsection{Replicator Dynamics}
\label{sec:replicatordynamic}
The Replicator Dynamic, one of most popular dynamics in 
evolutionary game theory, has a wide range of applications and 
attractive properties, such as population genetics and ecology, abiogenesis and
of course game theory \parencite[203]{hofbauer_evolutionary_1998}. 
It was mathematically formulated by \textcite{taylor_evolutionary_1978}, 
the biological concept replicator was introduced in 1982 by Dawkins 
\parencite{dawkins_extended_2016}.
As discussed above, the replicator dynamic can be motivated by different 
revision protocols. Using matrix notation, one finds 
for the strategies $j \in \{1,2\}$
\begin{alignat}{2}
        \dot{x}_j &= x_j\left(\left(x^T A\right)_j -
                \left(x^T A x\right)\right). 
        \label{eq:replicator}
\end{alignat}
The lack of the factor $2$, comparing it with equation 
\eqref{eq:replicatorrev}, is due to the derivation. In fact, any
positive transformation of the payoff matrix and hence the payoff function
results only in a change in speed in the replicator dynamic
\parencite[73]{weibull_evolutionary_1997}.
The growth of the share $\dot{x}_j(t)$ at time $t \in \realnumb$ depends 
on the current share in the population of strategy $j$ and the 
excess payoff of that strategy, the difference between the expected 
payoff to strategy $j$ against the current state of the population and the
average payoff across the whole population.
Intuitively, the share of a strategy in the population increases (decreases) 
if the strategy yields a higher (lower) than average payoff. 
This property of a (deterministic) evolutionary dynamic is called 
\textit{Positive correlation} \parencite{sandholm_population_2010}. 
\todo{pagenumber}
Another property of the replicator dynamic is the low data requirement. As 
seen in the previous section, it is not necessary to assume that an agent
gets to know the payoffs of all agents in the population.
It was sufficient to assume that he gets to know the payoff of one random 
player. 
Contrary to the interpretation that agents with such revision protocols 
are "boundedly rational", \textcite{gintis_game_2000}  argues 
that ``this is very misleading, because the real issue is the 
distribution of the information'' \parencite[273]{gintis_game_2000}. 
The argument goes, that even if they were not ``bounded'' they 
could not calculate since they do not have more information. 
One drawback of the replicator dynamic for certain applications, especially
for biologists, is the lack of mutation, a random error in the replication 
process.\todo{cite hofbauersigmund}
 Indeed, the deterministic replicator dynamic does not incorporate 
any such effect. A strategy that has not existed or has vanished from the 
population will not be in a future state of the population. 
That means, that if in a population state only a 
subset $S' \subset S$ of the 
set of pure strategies is used, any future population state can also only 
contain strategies from this subset $S'$. Hence, if a strategy in 
the stag hunt game vanishes, the population state is in one of the two pure 
Nash equilbria. It is practical for further use to express the replicator 
dynamics for the parametrized SH game using the matrix in equation 
\eqref{eq:matrix} and define $x_1(t) := x,\ x_2(t) = 1-x$:
\begin{alignat}{2}
        \label{eq:replicatorpara}
        \dot{x} := \varphi(x) &= x^2(a-b+2d-2c) - x^3(a-b+d-c) -x(d-c) \\
                              &= x^2(\alpha_1+2\alpha_2) 
        - x^3(\alpha_1+\alpha_2) - x(\alpha_2)
\end{alignat}
The last step follows by using local shifts to the matrix $A$ from section
\ref{sec:traditionalconcepts}. This does not change the dynamic at all 
\parencite[73]{weibull_evolutionary_1997}. The polynominal $\varphi(x)$ is
of degree $3$ and contains all information about the dynamic. Actually, 
outlined in the following, it tells which state the population converges to.
For the parameter set $a=5, b=3, c=0, d=2$ the graph of $\varphi(x)$ is 
plotted in figure \ref{fig:polynominal}. 
\todo{Description of the graph, roots e.t.c.}
\ref{fig:polynominal}.
\begin{figure}[h]
        \centering
        \includegraphics[scale=0.5]{polynominal.pdf}
        \caption[Polynominal of the Replicator Dynamic]{$\varphi(x)$ for the parameter setting $a=5, b=3, c=0, d=2$}
        \label{fig:polynominal}
\end{figure}
For the purpose of analyzing the properties of the replicator dynamic, it is 
useful to introduce some vocabulary used in dynamical system theory. 

A solution through an initial state $x_0$ of the dynamical system is called a 
\textit{trajectory}, whereby it is distinguished between a forward trajectory 
for $t \rightarrow \infty$ and backward trajectory for $t \leftarrow \infty$.
Analytical solutions, i.e. a solution one can express with the use of
basic functions, can only be derived for simple cases. 
It is easier to solve the differential equation numerically using some
computer software. 
However, the existence and uniqueness of a solution through
any initial state for the replicator dynamic is guaranteed by the theorem 
of \textit{Picard-Lindel\"of}, theorem 6.1 in 
\textcite[74]{weibull_evolutionary_1997}. 

The natural question that arises once a solution is obtained, is whether there 
is a connection between the evolutionary dynamic and the equilibria in the 
stage game. To see this, consider the concept of a fixed point.
A point $x^* \in \realnumb^2$ of a dynamic system, such as the replicator 
dynamic in \eqref{eq:replicator}, is called a fixed point
if $\dot{x}(t) = \varphi(x^*) = 0$ i.e. it stays in the current state for all 
$t \rightarrow + \infty $. 
Analyzing \eqref{eq:replicatorpara}, this happens if either a strategy
is not present in the population or the excess payoff of a strategy is zero. 
In the stag hunt game, the fixed points of the
dynamic coincide with the Nash equilibria of the stage game, 
$\varphi(x^*) = 0$ for $x^* \in \{0,\frac{\alpha_2}{\alpha_1+\alpha_2},1\}$. 
It suffices to calculate the roots of the polynominal $\varphi(x)$, either
numerical or, as the Nash equilibria are usually calculated beforehand,
reducing the degree by polynominal division. In general, the replicator 
dynamic lacks the \textit{Nash stationarity} property and hence there are 
games with fixed points which are not Nash\footnote{For a trivial example, 
consider the prisoners dilemma in \ref{fig:pd}. A population only consisting of 
cooperating agents stays in that state, which is not a Nash equilibrium even 
though it is Pareto-dominating.} \parencite{sandholm_population_2010}.
Concerning fixed points, one is usually interested if the fixed points of 
a dynamic system are stable in terms of some disturbance of the system. 
There are different approaches to stability. A quite strict and useful one 
in the stag hunt game is \textit{asymptotic stability}. 
It essentially requires that a system in a specific fixed point returns 
to the fixed point after a perturbation happened.
Hence, the system tends to move back to an asymptotically stable fixed point
once disturbed. Formally, a $\epsilon$-perturbation of 
\eqref{eq:replicatorpara} is a trajectory of the system with the initial
condition $x_0$ at some ball $B_\epsilon(x)$ of radius $\epsilon >0$ and 
$x_0 \neq x^*$. The fixed point $x^*$ is then called \textit{asymptotically
stable} if there exists an $\epsilon$-perturbation for that $x(t) \rightarrow
x^*$ as $t \rightarrow \infty$. 
The \textit{basin of attraction} of a fixed point $x^*$ is defined as the set 
of points $x_0 \in \realnumb$ for which a trajectory through $x_0$ approaches 
the fixed points $x^*$. In the one dimensional case, this is simply the range 
of $x$ for that any solution to the differential equation with the initial 
conditions in this range converges to the fixed point. With the following 
theorem independently contributed by \cite{hartman_lemma_1960} and 
\cite{grobman_homeomorphism_1959}, one can easily 
find the asymptotically stable fixed points in the stag hunt game:
\begin{mydef}
        If a one-dimensional dynamical system $\dot{x}(t) = \varphi{x}$ 
        has a hyperbolic fixed point $x^*$, $x^*$ is asymptotically stable
        if its linearization 
        $\dot{x}(t) = \frac{\partial\varphi(x)}{\partial x} := \varphi'(x)$ 
        is asymptotically stable. 
        A fixed point is called hyperbolic in one-dimension if 
        $\varphi'(x^*) \neq 0$ and the linearization is stable at that
        point if $\varphi'(x^*) < 0$.
\end{mydef}
The solution to the linearization can be analytical derived by 
a standard tool for differential equation - seperation of variables and 
integration.
The linearization around the fixed point $x^*$ is accordingly
$x(t)= x(0) e^{\varphi'(x^*)t}$, where $x(0)$ denotes the 
initial condition, i.e. the share of agents choosing strategy 1 in the 
beginning.
Applying the theorem to \eqref{eq:replicatorpara}, one finds that the 
fixed point $x=0$ corresponding to the Nash equilibrium of hunting hare, is 
asymptotically stable, since $\varphi'(0) = - \alpha_2 <0$. 
The linearization around the fixed point is $x(t) = x(0) e^{-\alpha_2 t}$, 
which clearly approaches zero as $t \rightarrow \infty$. 
Similarly, the Nash equilibrium 
of hunting stag at $x=1$ is a asymptotically stable fixed point as
$\varphi'(1) = -\alpha_1 <0$ with linearization $x(t) = x(0) e^{-\alpha_1 t}$.
However, the linearization theorem cannot be used for the mixed Nash 
equilibrium, hence the dynamical system is not hyperbolic there, 
$\varphi'(\frac{\alpha_2}{\alpha_1+\alpha_2}) = 0$. Nevertheless, suppose 
there is some $\epsilon$-perturbation of the system  \todo{Voooodoooo}
 with $x_0= \frac{\alpha_2}{\alpha_1+\alpha_2}+ \epsilon$ and 
 $\frac{\alpha_1}{\alpha_1+\alpha_2} > \epsilon > 0$. Plugging this into
 \eqref{eq:replicatorpara} one sees that $\dot{x} >0$, the population share
 grows monotonically for every $\epsilon$, converging to the fixed point 
 $x = 1$. Conversely, for $0 > \epsilon > \frac{\alpha_2}{\alpha_1+\alpha_2}$,
 $\dot{x} < 0$ and so the population share of agents 
 playing strategy 1 decreases monototically, until it reaches the 
 fixed point $x=0$. This is also shown in figure \ref{fig:basinofattraction}.
 \todo{Better description of the graphic}
 The horizonal line marks the mixed-strategy fixed point at $75\%$ for the
 parameter setting $\alpha_1 = 1$ and $\alpha_2=3$. If the population starts
 at this fixed point it by definiton stays there without any disturbance. 
 But any invasion of individuals with a uniform strategy leads away from
 this fixed point, resulting in convergence to one of pure strategy equilibria.
 The use of the word invasion is not by coincidence. It clearly shows the 
 connection of the evolutionary stable strategy of the stage game to the 
 stable fixed points of the dynamic. In fact, one can prove that this is not
 only true in the simple stag hunt, but it holds also under general settings.
 \todo{Folk theorem of evolutionary game theory}
\begin{figure}
        \centering
        \includegraphics[scale=0.5]{basinofattraction.pdf}
        \caption[Replicator dynamic of the stag hunt game]{Replicator dynamics in the stag hunt game for 
                $\alpha_1=1,\ \alpha_2=3$ with different initial conditions}
                \label{fig:basinofattraction}
\end{figure}

Concering the equilibrium selection, the evolutionary approach with replicator
dynamic has a definite answer. The population will reach one of the three
Nash equilibria depending on the initial condition. 
However, an equilibrium not asymptotically stable is rather implausible,
because it only emerges from one initial condition, where the population
is already exactly in that population state. 
Intuitively, one additional agent playing
a pure strategy suffices to get the population state moving to one of the
pure equilibria. As the model outlined here is only a deterministic
approximation, any stochastic shock would lead to such a disturbance and hence
would start convergence to one of the states with evolutionary stable 
strategies.
The population converges to the stag hunting or the hare
hunting equilibrium if the initial conditions lie in their basin of attraction.
As discussed, the population converges to the state $x=1$, the stag hunting 
equilibrium of the stage game, for all initial conditions 
$x_0 \in (\frac{\alpha_2}{\alpha_1+\alpha_2},1]$. Any trajectory of the 
dynamical system with initial conditions 
$x_0 \in [0,\frac{\alpha_2}{\alpha_1+\alpha_2})$ lead to the hare hunting
equilibrium. Interestingly, one observes another connection of the 
dynamic with the stage game. The basin of attraction is larger for the
risk-dominating equilibrium. By definition \eqref{eq:riskdom}, 
the hare hunting equilibrium risk-dominates if $\alpha_2 > \alpha_1$,
so in fact the basin of attraction is larger. With the parameter setting shown
in figure \ref{fig:basinofattraction},$75\%$ of the possible initial 
conditions lead to the risk-dominant equilibrium, whereas only $25\%$ of the 
initial conditions lead to the payoff-dominant equilibrium. 
The equilibrium selection is determined by the position of the initial
condition. However, it is not clear what specifies the initial conditions. 
If for example the agents choose a random strategy in a first round of the 
game, independently of each other, one can say that the equilibrium with the
larger basin of attraction, the risk-dominant equilibrium, is more likely 
reached. \todo{what does evolutionary game theory tell about initial condition} 
Giving an exact answer which equilibrium will be selected through the dynamic
process, this approach still leaves us with the problem of what agents 
choose in a first round one-shot game. Section \ref{sec:experimentalevidence} 
summarizes evidence found in laboratory stag hunt game concerning the 
one-shot game strategy choice and so the justification for initial conditions.

\section{Coordination and network externalities}
\label{simplemodel}
This section investigates the effect of a network externality on the
evolutionary stag hunt game.
To motivate the idea, consider students in university being assigned to
projects in their classes with other students. Nowadays, most of them 
involve some kind of software tool to either present the results, analyze 
the subject or just manage the process of aggretating 
knowledge. Usually the choice of the tools can be simplified to two 
possibilities. One can either use proprietary software
which is usually more user-friendly, but costly due to relative high 
license cost for either the student or the institution using it. 
Beyond that, students can choose to use open source software which is freely
available and can be customized, contrary to proprietary software,
for the personal needs. Common examples for this are Latex or Open Office
vs. Microsoft Word for text editing, Python vs. Matlab for scientific 
computing and R vs. Stata for statistical analysis.
For simplification, I assume that during their studies, a large body of 
homogenous students is randomly paired by their professor into groups of two 
for a project, implicitly playing a coordination game in which they have two 
strategies available. The students contribute to the project 
independently, using either open source software, 
represented by strategy 1 or proprietary software, i.e. strategy 2.
If both players choose strategy 1 they receive 
the utility $a$. Using different software, the player with 
proprietary software receives the utility $c$ and
the player contributing with open source software gets $b$.
Due to the high customizability to their projects, coordinating on open
source software results in the highest utility $a>c>b$.
When students fail to coordinate, the ``costs'' are beared
by the one using open source, as portability is usually only possible in 
this direction, receiving only $b<c$.
Coordination on proprietary software leaves both of them with $d$, which 
satisfies, despite the success to coordinate, $a> c\geq d>b$, as they have to
pay for the licenses. 
The structure of the game is precisely that of a stag hunt game. 
Coordination on open source software is the payoff-dominant Nash equilibrium,
but it is risk-dominated by the equilibrium where the students coordinate on
proprietary software. 
Concerning the behavior, I assume that they are not perfectly rational and
do not adjust their software choice each time they have to do a project.
For simplicity, students in this model behave according to the 
revision protocol of pairwise proportional imitation discussed in section 
\ref{sec:revisionprotocols}. 
This is justified, as students tend to use software they always used and 
only change it, if they get to know someone being more successful with
another software choice. Therefore, the dynamic in the model can be 
described by the replicator dynamic derived for the stag hunt game in 
equation \eqref{eq:replicatorpara}.
This model would not differ in terms of convergence and stability of Nash 
equilibria to the evolutionary stag hunt game above.

However, I want to introduce ``network externalities'' in this model. 
The interest of the economic literature turned to open source software 
development, because it is commonly observed that users contribute to 
projects without receiving a monetary payment for it.
In fact users share their source code publicly to the open source community,
being motivated by peer recognition and the signaling for career concerns
a programmer can achieve \parencite[21]{lerner_simple_2002}.
The utility for a user then increases with the size of the community as 
more people contributing code increases the variety and usefulness of the 
software. 
To incorporate that in the model, let the utility for a student 
in the case of coordination on open source software be a function of 
population size using that strategy, $f(x)$. 
Preserving the general structure of the
game, it is useful to focus on the functional specification 
$f(x) = a + e(x)$, where $a$ is the payoff parameter used 
previously and $e(x)$ denotes the network externality. 
A positive network externality satisfes $e(x)>0$ and 
$\frac{de(x)}{dx}>0$, 
an increase of the population share choosing the strategy
also increases the utility due to the network. 
Including the parameter $a$ makes sure that the utility of this 
strategy combination is never below the utility of coordination on 
proprietary software. Otherwise, the structure of the stage game would 
differ fundamentally with respect to the population share choosing strategy 
one. So to say, the network externality just adds to the fact that open 
source software is cheaper and can be customized to the individual project.
The payoff matrix, using local shifts, than is 
$A=\begin{psmallmatrix}\alpha_1+u(x) && 0 \\ 0&& \alpha_2\end{psmallmatrix}$. 
Substituting into the replicator dynamic  \ref{eq:replicatorpara} one gets:
\begin{alignat}{2}
        \dot{x} = \varphi(x) = x^2(\alpha_1+e(x) +2\alpha_2 ) 
        - x^3(\alpha_1+e(x)+\alpha_2) - x(\alpha_2)
        \label{eq:externalitymodel}
\end{alignat}
Plugging $x=0$ and $x=1$ into equation \eqref{eq:externalitymodel} one can 
ensure, by applying the linearization theorem,
that the fixed point with pure strategies and their
property of asymptotical stability did not change.
($\varphi'(1) = -\alpha_1 -e(1),\ \varphi'(0) = -\alpha_2 -e(2)$).
While the literature on network externalities usually assumes a diminishing
marginal effect of the network externality $\frac{d^2e(x)}{dx^2} <0$ 
\parencite[73]{lin_impact_2008}, I will 
focus on the linear form $e(x) = \beta x$, with the externality parameter
$\beta \in \realnumb_+$. This restriction does not change the main implication
of the model, but makes it less tedious to derive it.
Although not changing the stability of the pure states, 
the externality affected the inner fixed point of the dynamic.
The polynominal $\varphi(x)$ is now of degree $4$:
\begin{align}
        \dot{x} = \varphi(x) = -\beta x^4 -x^3(\alpha_1 + \alpha_2 
        - \beta ) + x^2 (\alpha_1 + 2 \alpha_2) - x(\alpha_2)
\end{align}
Using the knowledge from the general case, we can find the roots, i.e. fixed
points of the dynamic, by reducing the degree with the two roots known. The
polynominal of degree $2$ can then be solved applying a quadratic formula\footnote{
The solution with negative sign was omitted, since it has no economic 
interpretation.}, so that the fixed point with a mixed population state is
$x_3 = -\frac{\alpha_1+\alpha_2}{2 \beta} + 
\sqrt{(\frac{\alpha_1+\alpha_2)^2}{4\beta^2} +\frac{\alpha_2}{\beta}}$. 
Some arithmetic can verify that $0<x_3<\frac{\alpha_2}{\alpha_1+\alpha_2}$, 
the new inner fixed point is in the feasible range of the population share 
variable.
Taking the limit, $\lim_{\beta \rightarrow 0} x_3 = 
\frac{\alpha_1}{\alpha_2+\alpha_1}$, approves the intuition that for a 
diminishing network externality the model without externality is obtained.
The plot of the dynamic and the polynominal is 
shown figure in \ref{fig:plotmodellinear}.
Note that the share of students choosing strategy 1 is now lower at the 
inner fixed point, showing that the externality has a positive 
effect on the attractiveness of choosing open source software. Indeed,
the basin of attraction of the equilibrium in which all students are using 
open source software became larger, depicted by the distance to the
inner fixed point.
Comparison of the dashed, red horizontal line, the inner fixed point without 
externality, with the dashed, green horizontal line, the inner fixed point 
with externality, in figure \ref{fig:dynamiclinear}, illustrates this. 
\begin{figure}[h]
        \centering
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{polynominallinearmodel.pdf}
        \caption[Polynominal of the externality model]{The polynominal $\varphi(x)$} 
        \end{subfigure}%    
        \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[scale=0.5]{dynamiclinearmodel.pdf}
        \caption[Replicator dynamic of the model with externality]{The dynamic $x(t)$ for different $x(t=0)$} 
        \label{fig:dynamiclinear}
        \end{subfigure}%        
        \caption[Polynominal and Dynamic of the model with externality]{The linear externality model for parameters $\alpha_1=1,\alpha_2=3,
        \beta=3$}
        \label{fig:plotmodellinear}
\end{figure}
The basins of attraction have equal size for $\beta^* = 2 (\alpha_2 -\alpha_1)$,
attracting an equal amount of initial conditions. However,
the proprietary equilibrium looses its risk-dominance property 
only for $1 > x > \frac{\alpha_2-\alpha_1}{\beta}  $, using definition 
\ref{eq:riskdom}. The right hand side of the equation must be lower than one 
to change the risk-dominance property. This leads to the conclusion 
that in this model, the size of the basin of attraction and the risk-dominance
property do not coincide in general, since the externality changes the Nash 
product associated with the pure strategy 1 equilibrium. Consider for example 
the parameter setting $\alpha_1 =1, \alpha_2=3$, but now let $\beta=3$. 
The basin of attraction is not equal, since $3<\beta^*= 4$. 
The open source equilibrium attracts all trajectories starting within 
$x_0 \in (\frac{-2+\sqrt{13}}{3},1]$ and hence the proprietary equilibrium attracts
all trajectories $x_0 \in [0,\frac{-2+\sqrt{13}}{3})$. Clearly, the latter basin of 
attraction is larger ($\frac{-2+\sqrt{13}}{3}>0.5)$, 
but the open source equilibrium gets risk-dominant for
$x>\frac 23$. Implying that the network externality not only increases the
utility obtained from the choice of open source software, but also,
combined with a high enough user base, makes this choice risk-dominant. 

The model presented here is quite a simplification of the real world. 
First, the students are expected to be able to review which software is 
appropriate for their projects, hence have more information available then 
assumed in the model. Secondly, talking to their fellow student seems
possible before deciding which software to use for the project. 
However, as the experimental literature, discussed in 
\ref{sec:experimentalevidence}, and \textcite{aumann_nash_1990} suggests, 
cheap talk does not generally solve the coordination problem. 
Additionally, the way the attractiveness of a strategy increases with
the population size, the network externality, is modeled quite naive.
Students in university, and social networks in general, are not fully
connected, as students, for example, only do projects with fellow students
in the same classes. Furthermore, students usually try to do projects
with people they are socially connected with. In fact, the assumption about
a fully connected network is ``one of the main criticisms of evolutionary game
theory'' \parencite{hanauske_evolutionare_2011}. 
As a consequence, many researchers recently gave attention to the 
structure of the network underlying the interaction of agents and the 
evolving of this structure \parencite[46]{szabo_evolutionary_2007}.
For example \textcite{ohtsuki_simple_2006} found for a range of 
network topologies, that fewer connectivity 
leads to more cooperation, giving the intuition 
``The fewer friends I have the more strongly my fate is bound to theirs'' 
\parencite[1]{ohtsuki_simple_2006}.
Although the model with a positive linear network externality is far to 
simple to capture the effect of complex, adapting networks, it gives an 
intuition of how coordination of a small group, in this case two players, 
can be affected by the choice of the whole population.

\section{Experimental evidence}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Experimental section
%
\input{experimental.tex}
%
%
%
%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Outlook}
%%%%%%
%
% Summary and Outlook
%
%%%%%%
\input{summary.tex}
%
%
%



\section{Appendix}
\begin{figure}[H]
\begin{subfigure}{\textwidth}
\caption{Payoff matrices of games in \textcite{battalio_optimization_2001}}
        \label{fig:payoffbattalio}
        \centering
\begin{alignat*}{3}
        A_{2R} &= \mqty(45 & 0 \\ 35 & 40) \quad & &A_{R} = \mqty(45 & 0 \\ 
        40 & 20) \quad & &A_{0.6R} = \mqty( 45 & 0 \\ 42 & 12) \\
        \delta_{2R} &= 50  \quad & &\ \delta_{R} = 25 \quad & &\ \delta_{0.6R} = 15
\end{alignat*}
\end{subfigure}
\vspace*{50px}
\begin{subfigure}{\textwidth}
        \caption{Payoff matrices of the games used in \textcite{schmidt_playing_2003}}
        \centering
\label{fig:payoffschmidt}
\begin{alignat*}{3}
        A_{Game1} = \mqty( 100 & 20 \\ 60 & 60), \quad A_{Game2} 
                               &= \mqty( 100 & 20 \\ 80 & 80), \\ 
        A_{Game3} = \mqty( 100 
             & 60 \\ 80 & 80), \quad A_{Game4} &= \mqty( 100
                             & 0 \\ 80 & 60)
        \end{alignat*}
\end{subfigure}
\vspace{50px}
\begin{subfigure}{\textwidth}
        \caption{Payoff matrices of the games used
        in \textcite{dubois_optimization_2012}}
        \centering
        \label{fig:payoffdubois}
        \begin{alignat*}{3}     
                A_{Game1} &= \mqty(45 & 0 \\ 42 & 12),
                A_{Game2} &= \mqty(40 & 20 \\37 & 32),
                A_{Game3} &= \mqty(44 & 4 \\ 38 & 28) \\
                RR_{Game1}&= \frac 23, RR_{Game2}&=\frac 14, RR_{Game3}&=
                \frac 14
        \end{alignat*}
\end{subfigure}
\caption[Games in the laboratory experiments]{Payoff matrices for the 
games used in the experimental literature}
\end{figure}
\printbibliography
\newpage
\thispagestyle{empty}
\section*{Ehrenw\"ortliche Erkl\"arung}
\noindent
Ich versichere hiermit, dass ich die vorliegende Arbeit 
selbst\"andig und ohne Benutzung anderer als der angegebenen  Quellen  
und Hilfsmittel verfasst habe. W\"ortlich  \"ubernommene  S\"atze  oder  
Satzteile  sind  als  Zitat  belegt, andere Anlehnungen, hinsichtlich Aussage 
und Umfang, unter Quellenangabe kenntlich gemacht. Die Arbeit hat in gleicher 
oder \"ahnlicher Form noch keiner Pr\"ufungsbeh\"orde vorgelegen und ist nicht 
ver\"offentlicht. Sie wurde nicht, auch nicht auszugsweise, f\"ur eine andere 
Pr\"ufungs oder Studienleistung verwendet.
\vspace{1cm}

\noindent
Frankfurt am Main, den  \hspace{3cm} Unterschrift:
\end{document}







